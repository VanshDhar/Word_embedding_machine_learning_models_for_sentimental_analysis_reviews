{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support as score\n",
    "from sklearn.svm import LinearSVC as SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/vanshdhar/Desktop/amazon_reviews_us_Kitchen_v1_00.tsv'\n",
    "input_data = pd.read_csv(file_path, sep='\\t',error_bad_lines=False,warn_bad_lines=False)\n",
    "input_data =input_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Reviews:\n",
      "                                         review_body  star_rating\n",
      "0                Beautiful.  Looks great on counter.          5.0\n",
      "1  I personally have 5 days sets and have also bo...          5.0\n",
      "2  Fabulous and worth every penny. Used for clean...          5.0\n",
      "\n",
      "Reviews with 1.0 Rating: 426852 \n",
      "Reviews with 2.0 Rating: 241931 \n",
      "Reviews with 3.0 Rating: 349533 \n",
      "Reviews with 4.0 Rating: 731693 \n",
      "Reviews with 5.0 Rating: 3124553 \n"
     ]
    }
   ],
   "source": [
    "input_data = input_data[['review_body','star_rating']]\n",
    "\n",
    "#printing sample reviews and rating statistics\n",
    "print('Sample Reviews:')\n",
    "print(input_data.head(3))\n",
    "print('\\nReviews with 1.0 Rating: {} '.format((input_data.loc[ input_data['star_rating'] == 1.0 ]).shape[0]))\n",
    "print('Reviews with 2.0 Rating: {} '.format((input_data.loc[ input_data['star_rating'] == 2.0 ]).shape[0]))\n",
    "print('Reviews with 3.0 Rating: {} '.format((input_data.loc[ input_data['star_rating'] == 3.0 ]).shape[0]))\n",
    "print('Reviews with 4.0 Rating: {} '.format((input_data.loc[ input_data['star_rating'] == 4.0 ]).shape[0]))\n",
    "print('Reviews with 5.0 Rating: {} '.format((input_data.loc[ input_data['star_rating'] == 5.0 ]).shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of three classes- Reviews with Positive sentiment : 3856246, Reviews with Negative sentiment : 668783, Reviews with Neutral Sentiment : 349533,\n"
     ]
    }
   ],
   "source": [
    "#printing statistics of three classes\n",
    "rating_less_three = (input_data.loc[ input_data['star_rating'] < 3.0 ]).shape[0]\n",
    "rating_eq_three = (input_data.loc[ input_data['star_rating'] == 3.0 ]).shape[0]\n",
    "rating_more_three = (input_data.loc[ input_data['star_rating'] > 3.0 ]).shape[0]\n",
    "\n",
    "print('Statistics of three classes- Reviews with Positive sentiment : {}, Reviews with Negative sentiment : {}, Reviews with Neutral Sentiment : {},'.format(rating_more_three,rating_less_three,rating_eq_three))\n",
    "\n",
    "#labelling reviews\n",
    "input_data = input_data.loc[ input_data['star_rating'] != 3.0 ]\n",
    "input_data['binary_label'] = input_data['star_rating']\n",
    "\n",
    "\n",
    "input_data.loc[input_data['star_rating'] > 3.0, 'binary_label'] = 1\n",
    "input_data.loc[input_data['star_rating'] < 3.0, 'binary_label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random sampling positive and negative reviews\n",
    "positive_data = input_data.loc[ input_data['binary_label'] == 1 ]\n",
    "positive_data = positive_data.sample(n=100000, random_state=65)\n",
    "negative_data = input_data.loc[ input_data['binary_label'] == 0 ]\n",
    "negative_data = negative_data.sample(n=100000, random_state=65)\n",
    "\n",
    "input_data = pd.concat([positive_data,negative_data])\n",
    "input_data = input_data.sample(frac = 1, random_state=65).reset_index(drop=True)\n",
    "\n",
    "#splitting the dataset into training and testing\n",
    "training_dataset = input_data.sample(frac = 0.8, random_state=65)\n",
    "testing_dataset = input_data.drop(training_dataset.index)\n",
    "training_dataset = training_dataset.reset_index(drop=True)\n",
    "testing_dataset = testing_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample reviews before Data cleaning:\n",
      "\n",
      "1. So happy I made the purchase. The carbonating of water is much much easier with this one. I have an older version that you screw your bottles onto and this just saves so much time. Plus the indicator lights make it just to easy. Both features promote use for me which is a plus.\n",
      "2. I recommend this mug as a great alternative to getting cups every time you by a coffee/tea. It helps with recycling and environment.\n",
      "3. This strainer looks cool, but the metal rim around the top of the darned thing catches flecks of food when you go to dump it out. So it's kind of a bear to clean. Tell you the truth, I went back to my old enamel one from the 1930's. Nice try, WalterDrake.\n"
     ]
    }
   ],
   "source": [
    "#printing sample reviews before data cleaning and pre-processing\n",
    "sample_reviews_before_data_cleaning = training_dataset['review_body'].head(3)\n",
    "#print(training_dataset['review_body'].head(3))\n",
    "#print(testing_dataset['review_body'].head(3))\n",
    "print('\\nSample reviews before Data cleaning:\\n')\n",
    "for idx,rev in enumerate(list(sample_reviews_before_data_cleaning)):\n",
    "    print('{}. {}'.format(idx+1,rev))\n",
    "\n",
    "#printing average length of reviews before data cleaning\n",
    "training_data_Length_bef_clean = training_dataset['review_body'].str.len()\n",
    "testing_data_Length_bef_clean = testing_dataset['review_body'].str.len()\n",
    "\n",
    "avg_len_before_data = (sum(training_data_Length_bef_clean) + sum(testing_data_Length_bef_clean)) / (len(training_dataset)+len(testing_dataset))\n",
    "\n",
    "#Converting reviews in lower case \n",
    "training_dataset['review_body'] = training_dataset['review_body'].str.lower()\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing HTML tags\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "#removing URL tags\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "#training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['review_body'] = training_dataset['review_body'].str.replace('[^a-zA-Z ]', ' ')\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].str.replace('[^a-zA-Z ]', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: re.sub(' +', ' ', x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews in Dataset before Data Cleaning  : 321.8445, Average length of reviews in Dataset after Data Cleaning  : 306.76644\n"
     ]
    }
   ],
   "source": [
    "#!pip install contractions\n",
    "\n",
    "#performing contractions\n",
    "def Contractionfunction(text):\n",
    "    expanded_words = []     \n",
    "    for word in text.split(): \n",
    "        expanded_words.append(contractions.fix(word))    \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: Contractionfunction(x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: Contractionfunction(x))\n",
    "\n",
    "#printing average length of reviews after data cleaning\n",
    "training_data_Length_aft_clean = training_dataset['review_body'].str.len()\n",
    "testing_data_Length_aft_clean = testing_dataset['review_body'].str.len()\n",
    "\n",
    "avg_len_after_data = (sum(training_data_Length_aft_clean) + sum(testing_data_Length_aft_clean)) / (len(training_dataset)+len(testing_dataset))\n",
    "\n",
    "print('Average length of reviews in Dataset before Data Cleaning  : {}, Average length of reviews in Dataset after Data Cleaning  : {}'.format(avg_len_before_data,avg_len_after_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating average length of reviews before pre-processing\n",
    "training_data_Length_bef_process = training_dataset['review_body'].str.len()\n",
    "testing_data_Length_bef_process = testing_dataset['review_body'].str.len()\n",
    "\n",
    "avg_len_bef_process_data = (sum(training_data_Length_bef_process) + sum(testing_data_Length_bef_process)) / (len(training_dataset)+len(testing_dataset))\n",
    "#avg_len_testing_data = testing_data_Length_bef_process.sum() / len(testing_dataset)\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#removing stop words\n",
    "StopWords = stopwords.words('english')\n",
    "\n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: ' '.join([word for word in x.split() if word not in (StopWords)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews in  Dataset before Pre-Processing  : 306.76644, Average length of reviews in Dataset after Pre-Processing  : 188.24344\n",
      "\n",
      "Sample reviews after Data Pre-Processing:\n",
      "\n",
      "1. happy made purchase carbonating water much much easier one older version screw bottle onto save much time plus indicator light make easy feature promote use plus\n",
      "2. recommend mug great alternative getting cup every time coffee tea help recycling environment\n",
      "3. strainer look cool metal rim around top darned thing catch fleck food go dump kind bear clean tell truth went back old enamel one nice try walterdrake\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "#performing lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatize_tokens = [lemmatizer.lemmatize(w) for w in word_tokenize(text)]\n",
    "    return ' '.join(lemmatize_tokens)\n",
    "    \n",
    "training_dataset['review_body'] = training_dataset['review_body'].apply(lambda x: lemmatize_text(x))\n",
    "testing_dataset['review_body'] = testing_dataset['review_body'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "#training_dataset['review_body']\n",
    "\n",
    "#calculating average length of reviews after pre-processing\n",
    "training_data_Length_aft_process = training_dataset['review_body'].str.len()\n",
    "testing_data_Length_aft_process = testing_dataset['review_body'].str.len()\n",
    "\n",
    "avg_len_aft_process_data = ( sum(training_data_Length_aft_process) + sum(testing_data_Length_aft_process)) / (len(training_dataset) + len(testing_dataset))\n",
    "\n",
    "#printing average length of reviews before and after pre-processing\n",
    "print('Average length of reviews in  Dataset before Pre-Processing  : {}, Average length of reviews in Dataset after Pre-Processing  : {}'.format(avg_len_bef_process_data,avg_len_aft_process_data))\n",
    "#print('Average length of reviews in testing dataset after Pre-Processing  : {}'.format(avg_len_aft_process_data))\n",
    "\n",
    "#printing sample reviews after data cleaning and pre-processing\n",
    "sample_reviews_after_pre_processing = training_dataset['review_body'].head(3)\n",
    "\n",
    "#print(sample_reviews_before_data_cleaning)\n",
    "print('\\nSample reviews after Data Pre-Processing:\\n')\n",
    "for idx,rev in enumerate(list(sample_reviews_after_pre_processing)):\n",
    "    print('{}. {}'.format(idx+1,rev))\n",
    "#print(sample_reviews_after_pre_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(min_df=0.001)\n",
    "training_dataset['review_body']  = list(vectoriser.fit_transform(training_dataset['review_body']).toarray())\n",
    "testing_dataset['review_body']  = list(vectoriser.transform(testing_dataset['review_body']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Training Data Accuracy : 0.875175, Precision : 0.8565322695540676, Recall : 0.9014394242303079, F-score : 0.8784122732253744,  Perceptron Testing Data Accuracy : 0.865575, Precision : 0.8439488944415633, Recall : 0.896484375, F-score : 0.8694237353991112\n"
     ]
    }
   ],
   "source": [
    "X_train = training_dataset['review_body'].tolist()\n",
    "X_test = testing_dataset['review_body'].tolist()\n",
    "y_train = training_dataset['binary_label'].tolist()\n",
    "y_test = testing_dataset['binary_label'].tolist()\n",
    "\n",
    "perceptModel = Perceptron(tol=1e-3, max_iter = 75, random_state=0)\n",
    "perceptModel.fit(X_train, y_train)\n",
    "#accuracy = clf.score(X_test, y_test)\n",
    "y_train_pred = perceptModel.predict(X_train)\n",
    "y_test_pred = perceptModel.predict(X_test)\n",
    "\n",
    "precision_train,recall_train,fscore_train, _ = score(y_train,y_train_pred,average='binary')\n",
    "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
    "\n",
    "precision_test,recall_test,fscore_test, _ = score(y_test,y_test_pred,average='binary')\n",
    "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print ('Perceptron Training Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {},  Perceptron Testing Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {}'.format(accuracy_train,precision_train,recall_train,fscore_train,accuracy_test,precision_test,recall_test,fscore_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Data Accuracy : 0.90211875, Precision : 0.905415107886483, Recall : 0.898140743702519, F-score : 0.9017632557818606,  SVM Testing Data Accuracy : 0.89305, Precision : 0.8914670658682635, Recall : 0.8946814903846154, F-score : 0.8930713857228555\n"
     ]
    }
   ],
   "source": [
    "svclassifier = SVC(max_iter = 5000,random_state=0)\n",
    "#svclassifier = SVC(kernel='linear',tol=1e-3, max_iter = 500, random_state=0)#500\n",
    "svclassifier.fit(X_train, y_train)\n",
    "#y_pred = svclassifier.predict(X_test)\n",
    "y_train_pred = svclassifier.predict(X_train)\n",
    "y_test_pred = svclassifier.predict(X_test)\n",
    "\n",
    "precision_train,recall_train,fscore_train, _ = score(y_train,y_train_pred,average='binary')\n",
    "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
    "\n",
    "precision_test,recall_test,fscore_test, _ = score(y_test,y_test_pred,average='binary')\n",
    "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print ('SVM Training Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {},  SVM Testing Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {}'.format(accuracy_train,precision_train,recall_train,fscore_train,accuracy_test,precision_test,recall_test,fscore_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Data Accuracy : 0.9011, Precision : 0.9048832164657217, Recall : 0.8965163934426229, F-score : 0.9006803745826115,  Logistic Regression Testing Data Accuracy : 0.893175, Precision : 0.8922769307673082, Recall : 0.8939302884615384, F-score : 0.8931028444199834\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression(tol=2e-3, max_iter = 50, random_state=0)\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logisticRegr.predict(X_train)\n",
    "y_test_pred = logisticRegr.predict(X_test)\n",
    "\n",
    "precision_train,recall_train,fscore_train, _ = score(y_train,y_train_pred,average='binary')\n",
    "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
    "\n",
    "precision_test,recall_test,fscore_test, _ = score(y_test,y_test_pred,average='binary')\n",
    "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print ('Logistic Regression Training Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {},  Logistic Regression Testing Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {}'.format(accuracy_train,precision_train,recall_train,fscore_train,accuracy_test,precision_test,recall_test,fscore_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Training Data Accuracy : 0.86963125, Precision : 0.8662828845558651, Recall : 0.8743252698920432, F-score : 0.8702854975218741,  Naive Bayes Testing Data Accuracy : 0.866125, Precision : 0.860654523915297, Recall : 0.8731971153846154, F-score : 0.866880453426803\n"
     ]
    }
   ],
   "source": [
    "NBayesModel = MultinomialNB()\n",
    "NBayesModel.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = NBayesModel.predict(X_train)\n",
    "y_test_pred = NBayesModel.predict(X_test)\n",
    "\n",
    "precision_train,recall_train,fscore_train, _ = score(y_train,y_train_pred,average='binary')\n",
    "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
    "\n",
    "precision_test,recall_test,fscore_test, _ = score(y_test,y_test_pred,average='binary')\n",
    "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
    "\n",
    "print ('Naive Bayes Training Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {},  Naive Bayes Testing Data Accuracy : {}, Precision : {}, Recall : {}, F-score : {}'.format(accuracy_train,precision_train,recall_train,fscore_train,accuracy_test,precision_test,recall_test,fscore_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
